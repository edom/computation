\documentclass[10pt,statementpaper]{memoir}
\raggedbottom
\usepackage{cite}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{tikz-cd}
\theoremstyle{definition}
\newcounter{thmctr}
\newtheorem{mdef}[thmctr]{Definition}
\newtheorem{mque}[thmctr]{Question}
\newtheorem{mcon}[thmctr]{Conjecture}
\newtheorem{msco}[thmctr]{Strong Conjecture}
\newtheorem{mcor}[thmctr]{Corollary}
\newtheorem{mlem}[thmctr]{Lemma}
\newtheorem{mthm}[thmctr]{Theorem}
\newcommand\sno{\ensuremath{\mathcal N}}
\newcommand\syes{\ensuremath{\mathcal Y}}
\newcommand\Fin{\ensuremath{\operatorname{Fin}}}
\newcommand\powerset{\ensuremath{\mathcal{P}}}
\newcommand\fa[1]{\forall#1\,}
\newcommand\Fa[1]{\forall#1~}
\newcommand\FA[1]{\forall#1~~}
\newcommand\sfT{\mathsf{T}}
\newcommand\Typ{\mathsf{Typ}}
\newcommand\Either[2]{\mathsf{Either}~#1~#2}
\newcommand\Left[1]{\mathsf{Left}~#1}
\newcommand\Right[1]{\mathsf{Right}~#1}
\newcommand\Void{\mathsf{Void}}
\newcommand\Bit{\mathsf{Bit}}
\newcommand\Pair[2]{\mathsf{Pair}~#1~#2}
\newcommand\Bool{\mathsf{Bool}}
\newcommand\Bits{\mathsf{Bits}}
\newcommand\true{\mathsf{true}}
\newcommand\false{\mathsf{false}}
\newcommand\UDMach[1]{\mathsf{UDMach}~#1}
\newcommand\DMach[1]{\mathsf{DMach}~#1}
\newcommand\NMach[1]{\mathsf{NMach}~#1}
\newcommand\Mach[2]{\mathsf{Mach}~#1~#2}
\newcommand\Macha[1]{\mathsf{Mach}~#1}
\newcommand\RecRel{\mathsf{RecRel}}
\newcommand\List[1]{\mathsf{List}~#1}
\newcommand\Kleene[1]{\mathsf{Kleene}~#1}
\newcommand\RecFun{\mathsf{RecFun}}
\newcommand\DTM{\mathsf{DTM}}
\newcommand\NTM{\mathsf{NTM}}
\newcommand\sfDMach{\mathsf{DMach}}
\newcommand\sfNMach{\mathsf{NMach}}
\newcommand\Nat{\mathsf{Nat}}
\newcommand\sfFun{\mathsf{Fun}}
\newcommand\sfSet{\mathsf{Set}}
\newcommand\sfRel{\mathsf{Rel}}
\newcommand\sfVec{\mathsf{Vec}}
\newcommand\Vect[2]{\sfVec~#1~#2}
\newcommand\Set[1]{\sfSet~#1}
\newcommand\Relab[2]{\sfRel~#1~#2}
\newcommand\Fun[2]{\sfFun~#1~#2}
\newcommand\sfPred{\mathsf{Pred}}
\newcommand\Pred[1]{\sfPred~#1}
\newcommand\ob{\operatorname{ob}}
\newcommand\setB{\mathbb B}
\newcommand\decset{\mathcal D}
\newcommand\langset{\mathcal L}
% time complexity
\newcommand\TC{\mathcal{T}}
\newcommand\leTC{\ensuremath{\le_\TC}}
% time equivalence class
\newcommand\Teq{\mathbb{T}}
\newcommand\Teqsum{\mathbf{T}}
% space equivalence class
\newcommand\Seq{\mathbb{S}}
\newcommand\Seqsum{\mathbf{S}}
\newcommand\SC{\mathcal{S}}
% \newcommand\hom{\operatorname{hom}}
\newcommand\fite[3]{\text{if}~#1~\text{then}~#2~\text{else}~#3}
\newcommand\id{\ensuremath{\operatorname{id}}}
\newcommand\mE{\ensuremath{\mathcal E}}
\newcommand\mP{\ensuremath{\mathcal P}}
\newcommand\mM{\ensuremath{\mathcal M}}
\newcommand\mL{\ensuremath{\mathcal L}}
\newcommand\bB{\ensuremath{\mathbb B}}
\newcommand\amb{\operatorname{amb}}
\newcommand\ambc{\operatorname{ambc}}
\newcommand\fnull{\ensuremath{\operatorname{null}}}
\newcommand\fhead{\ensuremath{\operatorname{head}}}
\newcommand\ftail{\ensuremath{\operatorname{tail}}}
\newcommand\fcons{\ensuremath{\operatorname{cons}}}
\newcommand\aTIME{\operatorname{\alpha-TIME}}
\newcommand\idTIME{\operatorname{id-TIME}}
\newcommand\mPTIME{\operatorname{\mP-TIME}}
\newcommand\TIME{\operatorname{\mathsf{TIME}}}
\newcommand\DTIME{\operatorname{\mathsf{DTIME}}}
\newcommand\NTIME{\operatorname{\mathsf{NTIME}}}
\newcommand\EXPTIME{\ensuremath{\mathsf{EXP}}}
\newcommand\PTIME{\ensuremath{\mathsf{P}}}
\newcommand\NPTIME{\ensuremath{\mathsf{NP}}}
\newcommand\NSPACE{\operatorname{NSPACE}}
\newcommand\DSPACE{\operatorname{DSPACE}}
\newcommand\SDP{\ensuremath{\operatorname{SD}}}
\title{Theory of Computation (working title)}
\author{Erik Dominikus}
\date{2014-04-08T12:10:00+0700}
\begin{document}

\frontmatter

\begin{titlingpage}
\maketitle
\end{titlingpage}

\chapter*{Preface}

This book started out as a paper
as I set out to prove $\PTIME \neq \NPTIME$, not for the first time,
but things got out of hand quickly.
I realized that I lacked the necessary knowledge to even clearly define the problem,
and I was not comfortable with the current theory of computation
as I felt that the usage of Turing machines and set theory make the problem more difficult
than it ought to be.
I thought perhaps I could invent a theory of computation
where $\PTIME \neq \NPTIME$ is just an obvious corollary
of the axioms.
If that turns out to be impossible, perhaps I can transform the problem into something else.
Well, at least I tried.
If anything turns out to be impossible, at least we know why it is impossible,
so our descendants can try other paths in the future,
so they can avoid the dead ends that we have found.

The money did at first motivate me somewhat, and perhaps the fame,
but it was too soon to be happy about uncertain things.
After all, I was just a young impatient salaryman dreaming to be rich,
and I was not a professional mathematician by training.

The contribution of this book is the restatement
of theory of computation using a constructive type theory
similar to that of Martin-L\"of.
This book aims to show that it can be done,
and aims to translate only the basics of the theory of computation,
not the entire theory.

\newpage

\tableofcontents
\listoffigures

\mainmatter

\chapter{TODO}

cite CMI problem description;
understand Baker-Gill-Solovay (?) theorem and relativization;
read Rogers \cite{Rogers1987};
read Arora and Barak \cite{Arora2009};
read Marek and Remmel \cite{Marek2009};
read Boolos, Burgess, and Jeffrey \cite{Boolos2002}

Descriptive complexity: Fagin's theorem.
Neil Immerman's book.
NP equivalent to existential second-order logic.
% http://en.wikipedia.org/wiki/Fagin%27s_theorem

\chapter{Introduction}

A relation between two sets is a subset of the Cartesian product of those sets.
A \emph{relation} from $A$ to $B$ is a subset of $A \times B$.
A \emph{function} is a single-valued relation:
an injection:
If $a = b$ then $f a = f b$.
If $(a,b) \in f$ and $(a,c) \in f$ then $b = c$.

There are infinitely many functions.
What does it mean for a function to be \emph{computable} or \emph{recursive}?

Constant function $\forall x ~ f x = a$.

Natural number successor function $S$.

A \emph{recursive predicate} is a function having type $\mathbb N \to \{0,1\}$.
There is a bijection between $\{0,1\}^*$ and $\mathbb N$
so every function having type $\{0,1\}^* \to \{0,1\}$ can also be considPred{a} recursive predicate.
We say that $x$ \emph{satisfies} $p$ iff $px = 1$.
We say that $p$ is \emph{satisfiable} iff there exists $x$ that satisfies $p$.

The \emph{$p$-satisfaction problem}
and asks for the smallest $x$ that satisfies a given predicate $p$.
A related decision problem is the \emph{$p$-satisfiability problem} that
asks whether a given predicate $p$ is satisfiable.
This satisfiability problem allows us to show that $\PTIME \neq \NPTIME$
by constructing a satisfiable recursive predicate
$p$ in $\DTIME(\Theta n)$ such that
the corresponding $p$-satisfiability is in both $\NTIME(On)$ and $\DTIME(\Omega 2^n)$
where $n$ is the length of the shortest string that satisfies $p$.

An \emph{alphabet} is a finite countable non-empty set.

Many things can be \emph{recursive} or \emph{computable}: sets, functions, languages.

Blum \cite{Blum1967} defined a machine-independent complexity measure?
Blum speedup theorem implies $\EXPTIME = \PTIME$?

Rabin \cite{Rabin1977}?

Chow \cite{Chow1976} introduces the theory concisely.

Who? shows that partial computable functions are isomorphic to natural numbers.
This allows category theory to be used on computability theory.
An algorithm is a natural number.
An algorithm for simulating an algorithm: $\mathbb N \to \mathbb N$.
G\"odel numbering of partial recursive functions.
Formal systems.
Relates completeness, consistency, computability, decidability.

We use typed lambda calculus as our model of computation.

Informally, we construct a function $f$ such that $f$ is easy but $\SDP f$ is hard
such that there is no faster deterministic algorithm
than trying every possible subsequence.
A list of length $n$ has at most $2^n$ subsequences.

There is a bijection between $X^*$ and $\mathbb N$.
the empty string, 0, 1, 00, 01, 10, 11, 000, 001, 010, 011, 100, 101, 110, 111
\begin{align}
    \varphi [] &= 0
  \\ \varphi x &= \sum_{k=0}^{\mu x - 1} 2^k + \sum_{k=0}^{\mu x - 1} x_k \cdot 2^k
            \\ &= 2^{\mu x} - 1 + \sum_{k=0}^{\mu x - 1} x_k \cdot 2^k
\end{align}

Time hierarchy theorem?

The deterministic algorithm can use iterative deepening depth-first search:
\begin{align}
    sf &= \text{any $($map $f$ $\xi)$}
    \\
    \xi &= \text{fix} n []
    \\
    nx &= \text{map} (0:) x ++ \text{map} (1:) x
    \\
\end{align}
The nondeterministic algorithm:
\begin{align}
    sf &= gf[]
    \\
    gfx &= fx \vee \ambc (gf(0:x)) (gf(1:x))
\end{align}

\chapter{A summary of type theory}

\section{Motivation}

The type theory described here is inspired by
Martin-L\"of type theory
and the Haskell programming language.
The type theory is constructive (intuitionistic) and dependent.

This theory shall help us elucidate ideas that
would be otherwise cumbersome using set theory.
Perhaps this theory can help translate the P versus NP problem
into something easier.

\section{Notation}

Throughout this book, we will use the notation
\[ a : b \]
to mean that \emph{the type of $a$ is $b$}.
We can also say that $a$ \emph{inhabits} $b$
or that $a$ is an \emph{inhabitant} of $b$.

If $a$ is a type then $|a|$ is the cardinality of that type.
Informally, we can say that the cardinality of a type
is the size of a type, that is the number of inhabitants of that type.

Functions are \emph{curried}.
Juxtaposition (concatenation) like $f x$ means apply $f$ to $x$.
If $f : a \to b \to c$ and $x : a$ then $f x : b \to c$.
Partial application is possible.
The expression $f x y$ means $(f x) y$: apply to $y$ the result of applying $f$ to $x$.
Function application associates to the left.
Function type associates to the right.
The type $a \to b \to c$ means $a \to (b \to c)$.

\section{Meet the types}

\begin{itemize}
    \item
        $\Void$ is the empty type.
        It has no inhabitant.
        We will have something to do with this type.
    \item
        $\Bool$ is the type of boolean values.
        This type has two inhabitants: $\true$ and $\false$.
    \item
        $\Bit$ has two inhabitants: $0$ and $1$.
    \item
        $\Fun{a}{b}$ is the type of functions from $a$ to $b$.
        The function can be partial.
        This type is also be written $a \to b$.
    \item 
        $\Pred{a}$ is the type of
        logical predicates about objects of type $a$.
        We define
        \[ \Pred{a} = \Fun{a}{\Bool}. \]
    \item
        $\Nat$ is the type of natural numbers.
        This type is defined using Peano axioms.
    \item
        $\Set{a}$ is the type of the set of elements of type $a$.
    \item $\List{a} = \Kleene{a}$ is the Kleene closure of $a$.
        A list of $a$ is an ordered collection of elements of type $a$ with duplicates allowed.
        We define
        \[ \List{a} = \Fun{\Nat}{a} \]
        so we can write $[x,y,z] 1 = y$.
    \item $\Bits$ is the type of bitstrings.
        \[
            \Bits = \Kleene{\Bit}
        \]
    \item $\Either{a}{b}$ is the sum type or the union type
        that consists of $\Left{x}$ for all $x : a$
        and $\Right{y}$ for all $y : b$.
    \item $\Pair{a}{b}$ is the product type
        that consists of $(x,y)$ for all $x : a$ and $y : b$.
    \item $\Typ$ is the type of types.
        This implies
        \[ \Typ : \Typ \]
        (Does $\Typ : \Typ$ imply inconsistency?
        Russell's paradox?
        Unrestricted comprehension?)

        This means that $\sfSet$ and $\sfRel$ can be thought as the \emph{type functions}
        \begin{align*}
            \sfSet &: \Typ \to \Typ,
            \\
            \sfRel &: \Typ \to \Typ \to \Typ.
        \end{align*}
\end{itemize}

Function composition is $\circ : (b \to c) \to (a \to b) \to (a \to c)$.

The cardinality of $\Nat$ is $\aleph_0$ (aleph-null).

\section{Cardinality of types}

See also: cardinality, cardinal arithmetics,
aleph number, beth number, transfiniteness, Georg Cantor,
New Foundation, axiomatic set theory, Willard Van Orman Quine, type theory.

$|\List{a}| = |\Fun{\Nat}{a}|$.

To compute the cardinality of $\Fun{a}{b}$,
imagine that there are $|a|$ boxes.
Each box has exactly one out of $|b|$ possible contents.
The number of possible functions is thus $|b|^{|a|}$.

If a type has finitely many inhabitants,
then the cardinality of that type is the number of its inhabitants.

If there is a bijection between two types,
then those types have the same cardinality.

Type-theoretic restatement of Cantor's theorem?
There is no bijection between $a$ and $\Set{a}$.
$|a| < |\Set{a}|$.

Kind of ordering on cardinalities:
\begin{itemize}
    \item Iff there is an injection from $a$ and $b$, then $|a| \le |b|$.
    \item Iff there is an surjection from $a$ and $b$, then $|a| \ge |b|$.
    \item Iff there is a bijection between $a$ and $b$, then $|a| = |b|$.
\end{itemize}

Two sets are equinumerous (have the same cardinality) if and only if there is a bijection between them.
Given the cardinality of $A$, what is the cardinality of $A^*$?
There is a bijection between $\{0,1\}^*$ and $\mathbb{N}$?
Cantor's theorem?

$|\sfT a| = |\sfT (\Set{a})|$?

\section{Cardinality theorems}

\begin{mthm}
    \[
        |a| \lneq |\Set{a}|
    \]
\begin{proof}
    Has been proved by Georg Cantor using Cantor's diagonal argument
    that the cardinality of a set is strictly less than its power set.
    Beth numbers.
    $\beth_n \lneq \beth_{n+1}$ for each natural number $n$.
\end{proof}
\end{mthm}

\begin{mthm}[Equinumerosity among one-parameter types]
    For each $a$, all these types have the same cardinality:
    $\Pred{a}$, $\Set{a}$.
    \begin{proof}
        Let $p : \Pred{a}$ be a predicate and $s : \Set{a}$ be a set.
        We define $p$ and $s$ such that each object that satisfies the predicate $p$ is an element of the set $s$
        and also such that each element of the set $s$ satisfies the predicate $p$.
        \begin{align*}
            F p &= \{ x \,|\, p x \}
            \\
            G s &= \lambda x \to x \in s
        \end{align*}
        The relationship is
        \[ \FA{x} (p x \iff x \in s) \]

        But what if $p x = x \not\in S$.
        Or what if $p x = \neg\exists S ~ x \in S$?
        Or what if $p x = \Fa{S} x \in S$?
        Or what if $p x = x \in x$?
        What if $p x = \neg (p x)$?
        Isn't this prone to Russell's paradox?
        Unrestricted comprehension?
        FIXME?

        Or is this not prone?
        $p$ cannot refer to $s$?
        Can it?
    \end{proof}
\end{mthm}

Thus a predicate is a set and a set is a predicate.
It turns out that there is a name for this concept:
that set is the \emph{extension} of that predicate.
If $p$ is a predicate, then $p$ is also a set,
so we can write $x \in p$ to mean that $p x$ is true.
What if we assume that a predicate is equal to its own extension?
Now we make a bold but reasonable claim:
a predicate \emph{is} a set and a set \emph{is} a predicate.
This has some interesting consequences.

If we assume the equality, then $p$ becomes a fixed point of $\phi \mu$.
To see this, we have to define several functions.
Let $\phi$ be the flip combinator, that is $\phi f x y = f y x$.
Let $\mu$ be the set membership function, that is $\mu x y = x \in y$.
Recall that the $\eta$-reduction transforms $p x = q x$ to $p = q$.
\begin{align*}
    p x &= x \in s
    \\
    &= \mu x s
    \\
    &= \phi \mu s x
    \\
    p &= \phi \mu s
    \\
    p &= s
    \\
    p &= \phi \mu p
    \\
    p &= \phi \mu (\phi \mu p)
    \\
    &= \phi \mu (\phi \mu (\phi \mu p))
    \\
    &= \ldots
\end{align*}
That implies that we can write strange but provable things like these:
\begin{align*}
    1 \in \{0,1,2\} &= \{0,1,2\} 1 = \true
    \\
    3 \in \{0,1,2\} &= \{0,1,2\} 3 = \false
    \\
    (\lambda x \to x = 1) 1 &= 1 \in (\lambda x \to x = 1) = \true
\end{align*}
but this can be confusing at first.
Should we distinguish predicate and set?
Should we treat them as the same thing?
The membership operator $\in$ becomes swapped function application.

We can even generalize the notation $f x = x \in f$ to every function $f : a \to b$, not just predicates.
Let $f x = x + 1$. Then $f 0 = 0 \in f = 1$.
This may need some effort and time to get accustomed to,
but once you master it, you will be another mathematician.

\begin{mthm}[Equinumerosity among two-parameter types]
    All these types have the same cardinality:
    \begin{itemize}
        \item $\Relab{a}{b}$, $\Relab{b}{a}$
        \item $\Pred(a,b)$, $\Pred(b,a)$
        \item $\Set{(a,b)}$, $\Set{(b,a)}$
        \item $\Fun{a}{(\Set{b})}$, $\Fun{(\Set{a})}{b}$
    \end{itemize}
    \begin{proof}
        Proving $|\Relab{a}{b}| = |\Relab{b}{a}|$ is simple.

        Proving $|\Pred(a,b)| = |\Pred(b,a)|$ is simple.

        $r : \Relab{a}{b}$ and $p : \Pred(a,b)$ and $f : a \to b \to \Bool$.
        $r$ relates $x$ to $y$ if and only if $p(x,y)$ is true.
        \begin{align*}
            p z &= z \in r
             \\ &= \mu z r
             \\ &= \phi \mu r z
            \\
            p &= \phi \mu r
        \end{align*}
        Then let $p = r$.

        Since there is a bijection between $\Pred{(a,b)}$ and $\Relab{a}{b}$
        and between $\Pred{a}$ and $\Set{a}$,
        there is a bijection between $\Relab{a}{b}$ and $\Set{(a,b)}$.

        To prove that there is a bijection between $\Relab{a}{b}$ and $\Fun{a}{(\Set{b})}$,
        we choose any $r : \Relab{a}{b}$ that is a relation
        from objects of type $a$ to objects of type $b$.
        Define the \emph{image of $x$ in $r$} as
        $i r x = \{ y \,|\, \text{$r$ relates $x$ to $y$} \}$
        where the type of $i$ is $\Relab{a}{b} \to a \to \Set{b}$.
        We define the \emph{relation functionization} function $F$ as
        \[ F r = \{ (x,Y) \,|\, i r x = Y \} \]
        we capitalize $Y$ to highlight the fact that it is a set.
        $G : \Fun{a}{(\Set{b})} \to \Relab{a}{b}$ is the \emph{function relationization} function.
        \[ G f = \{ (x,y) \,|\, y \in f x \} \]
        $G f$ relates $x$ to $y$ iff $y \in f x$.
        We can see that $F(G f) = f$ and $G(F r) = r$.
        Thus $F \circ G$ is the identity of $\Fun{a}{(\Set{b})}$
        and $G \circ F$ is the identity of $\Relab{a}{b}$.
        Thus $F$ and $G$ are inverses of each other.

        ???
    \end{proof}
\end{mthm}

There is a mapping from $\Fun{a}{b}$ to $\Relab{a}{b}$.
There is a bijection between $\Relab{a}{b}$ and $\Relab{b}{a}$.
There is a bijection between $\Relab{b}{a}$ and $\Fun{b}{(\Set{a})}$.
This means that there is a bijection between $\Fun{a}{(\Set{b})}$ and $\Fun{b}{(\Set{a})}$.

\section{Relationship between type theory and graph theory}

A type $a$ with an endorelation $r : \Relab{a}{a}$
is isomorphic to a graph $(V, E)$
where $V$ is the set of all inhabitants of $a$
and $E : \Relab{a}{a}$.

Graph with uncountably infinite vertices.
Almost a topological space?

\chapter{A type-theoretic theory of computation}

\section{Recursive Function over Bitstrings}

Every inhabitant of $\Bit \to \Bit$ is recursive.

Every inhabitant of $\Bit \to \Bit \to \Bit$ is recursive.

For all $a$, the \emph{head} function $f [x 0, x 1, \ldots] = x 0$,
where $f : \List{a} \to a$, is recursive.

For all $a$, the \emph{tail} function $f [x 0, x 1, \ldots] = [x 1, \ldots]$,
where $f : \List{a} \to \List{a}$, is recursive.

If $f$ and $g$ are recursive, then so is every
well-formed typed lambda expression involving $f$ and $g$.

Every well-formed typed lambda expression is recursive?

\section{Fair Coding Scheme}

How to code almost anything into a bit string?

How to code members of type $t$ into bits?

How do we constrain the coding function so that meticulous mathematicians
cannot cheat by shifting the computation into the coding function?

Let $\mu t : t \to \Nat$ be the \emph{canonical measure} of an element of the countable type $t$.

Let $c t : t \to \Bits$ be the coding function.
This coding function must be measure-preserving, size-preserving.
Formally the coding function must satisfy
\[
    \mu t x \le \mu t y \iff \mu \Bits (c t x) \le \mu \Bits (c t y)
\]
for each $x : t$ and $y : t$.

\begin{figure}[h]
    \[
        \begin{tikzcd}
            x : t \arrow[d, "c t"] \arrow[r, "\mu t"] & \mu t x : \Nat \arrow[r, dotted, "\le"] & \mu t y : \Nat & y : t \arrow[l, "\mu t"] \arrow[d, "c t"]
            \\
            c t x : \Bits \arrow[r, "\mu \Bits"] & \mu \Bits (c t x) : \Nat \arrow[r, dotted, "\le"] & \mu \Bits (c t y) : \Nat & c t y : \Bits \arrow[l, "\mu \Bits"]
        \end{tikzcd}
    \]
    \caption{This figure probably contains mistakes}
\end{figure}

\section{Introduction}

A \emph{configuration} of a machine is a state of that machine at a certain time.
For example, a configuration of a Turing machine is a tuple of its state,
its head positions, and the contents of its tapes.
A \emph{machine} is a \emph{transition function}.
This transition function depends on the set of primitive operations of the machine.
Seen the other way around, this transition function
determines the set of primitive operations of the machine.

A machine is an embodiment of an algorithm.

A machine performs computation by repeatedly
making a transition from its current configuration
according to its transition function
until it reaches a terminal configuration.

A \emph{primitive operation} maps a configuration to a configuration.
Every primitive operation represents a computation that the machine can do in one unit time.
An \emph{architecture} is a set of primitive operations
and a set of rules for evaluating expressions built using those primitive operations.
A \emph{machine} is an architecture and a configuration representing its current state.

\section{Deterministic machines and graph theory}

The type of a deterministic machine is
\[ \DMach{c} = \Pair{(\Set{c})}{(\Fun{c}{c})} \]
where $c$ is the type of the configuration of the machine.
This machine is more general than a deterministic Turing machine.
To make a Turing machine,
the configuration must be countable
and the transition must be $\mu$-recursive.

Contrast the type of a deterministic machine to the type of a nondeterministic one
which differs only in the type of the second element of the pair:
\[ \NMach{c} = \Pair{(\Set{c})}{(\Relab{c}{c})} \]
Deterministic machines use $\sfSet$ while nondeterministic ones use $\sfRel$.
We can generalize this by parametrization to
\begin{align*}
    \Mach{t}{c} &= \Pair{(\Set{c})}{(t\,c\,c)}
    \\
       \sfDMach &= \Mach{\sfFun}{}
    \\
       \sfNMach &= \Mach{\sfRel}{}
\end{align*}
A \emph{deterministic machine} is $D = (I,\beta) : \DMach{c}$ where
$I : \Set{c}$ is the set of all \emph{initial configurations} of the machine
and $\beta : \Fun{c}{c}$ is the \emph{transition function}.
\begin{align*}
    \DTM &= \Pair{(\Set{\Bits})}{\RecFun}
    \\
    \NTM &= \Pair{(\Set{\Bits})}{\RecRel}
    \\
    \RecFun &< \Fun{\Bits}{\Bits}
    \\
    \RecRel &< \Relab{\Bits}{\Bits}
\end{align*}

If and only if $|\RecFun| = |\RecRel|$,
nondeterminism does not let the machine compute any more function.

Recall that a predicate is a set and a set is a predicate.
The set $I$ is a subset of $C$.
Let $C$ be the set of all inhabitants of the configuration type $c$.

We can see the tuple $(C,\beta)$ as a \emph{directed graph}
where each configuration is a vertex in the graph
and there is an edge from $x$ to $y$ if and only if $\beta x = y$.
An \emph{initial configuration} is a possible configuration from which the machine starts computing.
An initial configuration contains an input for the algorithm running on the machine.
A deterministic machine always starts computing from an initial configuration
and always either goes into infinite cycle or ends at terminal configuration.

\section{Termination}

We say that a configuration $x$ is \emph{terminal} iff
there is no $y$ satisfying $\beta x = y$, that is iff there is no $y$ such that $(x,y) \in \beta$.
In the graph, such configuration $x$ is terminal iff it has zero out-degree,
that is iff there is no edge from that configuration in the graph.

When there is ambiguity about which machine we are discussing,
we will write something like `terminal $D$-configuration'
or `$\beta$-terminal configuration' instead of just `terminal configuration'.

We say that a configuration $x$ \emph{eventually terminates}
iff there exists a natural number $n$ such that $\beta^n x$ is terminal.
Such configuration $x$ eventually terminates iff repeated application of $\beta$ to $x$
eventually produces a terminal configuration.
In the graph, such configuration $x$ eventually terminates iff
there is a path from $x$ to a terminal configuration.

\section{What do we mean by computation?}

A \emph{computation} is a path in the graph.
More precisely, a computation from $x_0$ to $x_{n-1}$
is a path $[x_0, x_1, \ldots, x_{n-1}]$
such that $x_{k+1} = \beta x_k$ for each natural number $k$ from $0$ to $n-1$.
We can see by induction that $x_k = \beta^k x_0$ for each natural number $k$ from $0$ to $n-1$.

A \emph{complete computation} is a computation that begins at an initial configuration
and ends at a terminal configuration.
Subcomputation is to computation as subpath is to path.
The graph can be seen as a set of complete computations.

We say that the deterministic machine $D = (I,\beta)$ \emph{computes} a function
$f : A \to B$
that is isomorphic to
$f_N : \mathbb{N} \to \mathbb{N}$
that is in turn also isomorphic to
$f_D : I \to T$
where $T$ is the set of all terminal configurations of the machine
where $f_D$ is defined as the following function:
\begin{equation}
    f_D = \{ (x,y) \,|\, x \in C \,\wedge\, x \text{ eventually terminates at } y \}
\end{equation}
Formally we say $f_D x = y$ iff there exists
a natural number $n$ such that $\beta^n x = y$ and $y$ is terminal.
The functions $p$ and $q$ are the \emph{input encoding} function
and the \emph{output encoding} function.
If we ignore the time used by the machine,
we can see the machine as a function from $I$ to $T$.
We picture this diagrammatically in Figure~\ref{f:comp}
that says $q \circ f = f_D \circ p$.

State every natural number $n$ as the sum of $p$ and $q$
where $p$ is the biggest prime less than $n$.

The encoding and decoding functions allow us to make
a distinction between a natural number
and the binary representation of natural number.

\begin{figure}[h]
\[
\begin{tikzcd}
    A \arrow[r, "f"] \arrow[d, "p"] & B \arrow[d, "q"]
    \\
    I \arrow[r, "f_D"] & T
\end{tikzcd}
\]
    \caption{The function computed by a machine}
    \label{f:comp}
\end{figure}

\section{A universal machine to simulate other machines?}

\[
    \UDMach{c} = \DMach{(\Pair{(\Fun{c}{c})}{c})}
\]

Machine or algorithm can be encoded as string.
\emph{Universal machines} simulate every machine.
A program corresponds to a \emph{partial computable function}.
Partial is not total.
Total function is a function defined for each element in its domain.
Partial function is a function that can be undefined for any number of elements in its domain.

The \emph{$c$-universal machine} of type $\UDMach{c}$ can compute every inhabitant of $\DMach{c}$.
The configuration type of the universal machine is $\Pair{(\Fun{c}{c})}{c}$.
\[
    I_u = \{ (\beta,x) \,|\, (I,\beta) : \DMach c \,\wedge\, x \in I \}
\]
Can a universal machine simulate all universal machines?
Let $t = (\Fun{c}{c},c)$.
Is $|t| = |(\Fun{t}{t},t)|$?
If and only if yes, then a universal machine can simulate all universal machines.

A universal deterministic machine is $U = (C_u, I_u, \beta_u)$
where the universal configuration set is $C_u = (C \to C, C)$.
A configuration of that universal machine is a tuple $(\beta, x)$.
This machine can compute everything every other machine can compute.
The universal machine transitions from $(\beta, x)$ to $(\beta, y)$
if and only if the simulated machine transitions from $x$ to $y$.
Let $u$ be the pairing function, that is $uab = (a,b)$.
Diagram \ref{f:univ} shows the relation:
\begin{equation}
    u \beta \circ \beta = \beta_u \circ u \beta
\end{equation}
The universal transition function $\beta_u$ can be seen as a set:
\begin{equation}
    \beta_u = \{ ((\beta,x), (\beta,y)) \,|\, (x,y) \in \beta \}
\end{equation}
The inverse of the partially applied pairing function $u a$ is $r$
where $r (a,b) = b$.

\begin{figure}[h]
\[
\begin{tikzcd}
    x \arrow[r, "\beta"] \arrow[d, "u \beta"] & y \arrow[d, "u \beta"]
    \\
    (\beta, x) \arrow[r, "\beta_u"] & (\beta, y)
\end{tikzcd}
\]
    \caption{A universal machine simulating a machine}
    \label{f:univ}
\end{figure}

\section{Nondeterminism from two deterministic machines}

The author got the idea of defining a nondeterministic machine
using two deterministic transition functions
from Arora and Barak \cite[p.~40]{Arora2009}.
We can form a nondeterministic machine $N$
from two deterministic machines $D_0 = (C,I,\beta_0)$ and $D_1 = (C,I,\beta_1)$
(both machines must have the same configuration set and the same initial configuration set)
as follows:
\begin{enumerate}
    \item The sets of initial configurations are the same.
    \item If $x$ is a configuration of $D_0$ then $(0,x)$ is also a configuration of $N$.
    \item If $x$ is a configuration of $D_1$ then $(1,x)$ is also a configuration of $N$.
    \item Nothing else is a configuration of $N$.
    \item $\beta = P 0 \beta_0 \cup P 1 \beta_1$ where $P c X = \{ (c,x) \,|\, x \in X \}$.
    \item $x$ is $\beta$-terminal iff $x$ is $\beta_0$-terminal or $\beta_1$-terminal or both.
    \item Nothing else is $\beta$-terminal.
\end{enumerate}

Sum types: $\Either{a}{b} = \Left a | \Right b$.
Iff $x : a$ then $\Left x : \Either{a}{b}$.
Iff $y : b$ then $\Right y : \Either{a}{b}$.
The configuration type is $\Either{c_0}{c_1}$.
\[
    I =
    \{ \Left x \,|\, x \in I_0 \}
    \cup
    \{ \Right x \,|\, x \in I_1 \}
\]
\begin{align*}
    (\Left x, \Left y) \in \beta \iff (x,y) \in \beta_0
    \\
    (\Right x, \Right y) \in \beta \iff (x,y) \in \beta_1
\end{align*}

$\NMach c$.
$\beta : \Relab{c}{c}$.
Does $\Fun{c}{c} < \Relab{c}{c}$ strictly?

We can also form a deterministic machine $D$ from a nondeterministic machine $N$.

A machine computes a function $f : X \to Y$ as follows:
For each $x$ such that $f x$ is defined, the string $\hat x$ (the string encoding of $x$)
is an initial configuration of the machine.
For each $y = fx$, the configuration $\hat y$ (the configuration encoding of $y$)
is a terminal configuration of the machine.
There is a path from $\hat x$ to $\hat y$.
There is a machine that computes this $f$ in unit time: the oracle of $f$.
Then there is another machine that computes it asymptotically slower.
Then there is another machine that computes it even slower asymptotically.
This goes on and on.
We can always invent a slower machine.

\begin{enumerate}
    \item
        Every configuration of $N$ can branch to \emph{at most two} configurations.
        Formally for each configuration set $X$, it holds that $\mu(B X) \le 2 \cdot \mu X$
        where $\mu X$ is the size of $X$.
\end{enumerate}

$\NMach c$.

\begin{figure}[h]
\[
\begin{tikzcd}
    x_0 \arrow[r, "\beta_0"] & y_0
    \\
    x \arrow[r, "\beta"] \arrow[u, "\pi_0"] \arrow[d, "\pi_1"] & y \arrow[u, "\pi_0"] \arrow[d, "\pi_1"]
    \\
    x_1 \arrow[r, "\beta_1"] & y_1
\end{tikzcd}
\]
    \caption{A nondeterministic machine built from two deterministic machines?}
\end{figure}

\section{Nondeterminism}

$N = (I,\beta) : \NMach c$
where $I : \Set{c}$ and $\beta : \Relab{c}{c}$.

Let $N$ be a nondeterministic machine $(C,I,\beta)$ where
Let $\beta \subseteq C \times C$ is the transition relation of $N$.
The tuple $(C,\beta)$ forms a graph.
There is an edge from $x$ to $y$ iff $(x,y)$ is in $\beta$.
with the following properties:
\begin{align}
    B^n X &= \{ (x,y) \,|\, x \in X \wedge (x,y) \in \beta^n \}
\end{align}
where $B X$ means $B^1 X$.

The relation $\beta$ can also be seen as an equivalent function $B : \powerset C \to \powerset C$
where $\powerset C$ is the power set of $C$.
Thus for every nondeterministic machine $(C, I, \beta)$
there is a deterministic machine $(\powerset C, \powerset I, B)$
that computes the same function at the same time.
But this deterministic machine is not countable.
Contradiction.
So nondeterminism bestows some power that deterministic machines cannot have.
There exists nondeterministic machine that cannot be simulated in equal time by deterministic machine.
(Is this a jump in logic? This seems wrong.)

But every deterministic machine can simulate
every nondeterministic machine by using breadth-first search.
So nondeterminism does not add any power?

Every deterministic machine is trivially a
nondeterministic machine since every transition function is a transition relation.

Is the set of functions computable by the nondeterministic machines
the same as that of deterministic machines?
Does it add any power?

\section{Usage of computational resources}

There are two computational resources we concern ourselves with:
time and space.
The unit of time is arbitrary.
Number of steps. Each step takes the same amount of time.
Each configuration transition uses the same amount of time.

The time used by that computation is the length of the corresponding path.
Formally the time required by the machine
to move from configuration $a$ to $b$ is
the smallest natural number $n$ such that $\beta^n a = b$.
By definition, the machine \emph{moves in one unit time} from configuration $x$ to $y$ iff $\beta x = y$
so $\beta$ depends on the set of primitive operations we equip the machine with.

The \emph{time-to-termination} of a configuration $x$,
written $\TC \beta x$,
is the smallest natural number $n$ such that $\beta^n x$ is terminal.
The time-to-termination of $x$ is the length of the shortest path from $x$ to a terminal configuration.

The maximum space usage of a computation is
the maximum of the space usages of the vertexes of the corresponding path.
Now suppose that we write $s x$ as the \emph{space usage} of configuration $x$.
The space usage of an initial configuration is the size of the input.
We define $\SC \beta x$,
the \emph{maximum-space-to-termination} of a computation from $x$
as follows:
\begin{equation}
    \SC \beta x = \max_{c \in C \beta x} s c
\end{equation}
where we define
$C \beta x = \{ \beta^k x \,|\, k \in \mathbb N \}$
as the set of all configurations
that can be reached from $x$ by following $\beta$.

\section{Time class}

What can a machine compute in a given number of steps?
Given more time, does a machine compute more?

What is the relationship between the size of the input and the running time?
What is the relation between the size of
an initial configuration and its time-to-termination?
How fast does the time-to-termination grow?

We are talking about a special kind of problems
whose time usage depends only on the length of the input.

A \emph{time class} is a set of all initial configurations that have the same time-to-termination.
Define the \emph{time class $n$ of machine $D$},
written $\Teq D n$ (or just $\Teq n$ when the machine is clear in the context),
as the set of every initial $D$-configuration
whose time-to-termination equals $n$.
Then define $\Teqsum n$, the \emph{max-time class $n$},
as the set of every initial configuration whose time-to-termination is less than $n$:
\begin{equation}
    \Teqsum n = \bigcup_{k = 0}^{n - 1} \Teq k
\end{equation}

For example, if the machine ignores its input and runs in constant time,
$\Teqsum 1 = I$.

\newcommand{\Cset}{\mathbf{C}}
Define $\Cset n$ as the set of every initial configuration whose size is less than $n$.
The \emph{time simplicity function of the machine} is
\begin{equation}
    K n = \frac{\mu(\Teqsum n)}{\mu(\Cset n)}
\end{equation}
(Find a better name?)
Higher $K n$ means faster computation.
The \emph{space simplicity function} of the machine is defined analogously:
\begin{equation}
    J n = \frac{\mu(\mathbf S n)}{\mu(\Cset n)}
\end{equation}

If the alphabet used is the binary alphabet $\{ 0, 1 \}$
and the input is a natural number encoded using the usual binary positional encoding,
then $\mu (C n) = 2^n$ and thus $\mu (\Cset n) = 2^{n + 1} - 1$.

If every instance $x$ of size $\mu x$ is solved by machine
(if the time-to-termination of each initial configuration $x$ of size $\mu x$ is less than $f(\mu x)$)
then we say that the problem is size-bounded by $f$.

Is there an $m$ such that for all $n > m$, we have $\Teq n = \varnothing$?
Only if the machine does not read all its input?

Now we examine the relation between the size of the initial configuration
and the time to termination of that configuration.
Intuitively, when the problem is non-trivial, as the initial configuration grows larger,
the machine should require more time to arrive at the related terminal configuration.
If the problem is non-trivial then a bigger initial configuration
should imply a higher time to termination of that configuration.

The minimum time-to-termination of every initial configuration of size $n$.

The maximum size of every initial configuration with time-to-termination of $n$.

The sets of functions computable by deterministic and nondeterministic machines are the same?

$\Teq n$ may be empty?

$\Teq n$ and the length of.

Define space equivalence class $n$ as $\Seq n$ as the set of
every configuration that has a maximum-space-to-termination of $n$.

Both $\TC$ and $\SC$ are functions.
Are they computable?
$C \beta$ is also a function.

Every configuration following $x_0$ is fully determined by $x_0$.
The computation is fully determined by its first configuration.
A machine is deterministic in the sense that given a configuration,
the computation that proceeds from that configuration is always the same.
What can graph theory say about computation?
What result in graph theory helps us discover something in computation theory?
Weighted graph can model computation with nonuniform transition time.
But for what?
Modeling modern machines with speculative execution?

We say that an algorithm $a$ is in $\TIME(O(f n))$ where $n$ is the length of the input iff
for infinitely many (but not necessarily every) $i$ in each initial configuration
$x \equiv (a,i)$ that eventually terminates, we have $\TC x \in O(f(s x))$.

Determining if a number is divisible by $m$ is a constant-time operation in a machine with alphabet of size $m$,
but is it also constant-time operation in a machine with another base that is coprime to $m$?

\section{Uncomputable functions}

Each machine has infinitely many functions that it \emph{cannot compute}.
We prove this by diagonalization.
Suppose that the set of functions computable by a machine is $F = \{ f_0, f_1, f_2, \ldots \}$.
Suppose also that there is no function outside that set.
Alas, we can define a function $f$
such that $f0 \neq f_0 0$, $f1 \neq f_1 1$, $f2 \neq f_2 2$, and so on
such that $fn \neq f_n n$ for each natural number $n$.
This $f$ is not equal to any function in $F$ so it cannot be in that set.
Contradiction.

Nondeterministic machine is a $\sigma$-algebra of deterministic machine?
Can we relate nondeterminism to topology?

We can make it a topological space?
Nondeterministic reversible computation?
Remember that a configuration of a nondeterministic machine is a set.
Allow the no-computation: for each configuration $x$, make $x \in N x$.
Then make it reversible:
if $y \in N x$ then $x \in N y$.
Then make each subset of $B x$ to be in $N x$.
We can $(C,N)$ where $C$ is the set of all configurations of the machine
and $N : C \to 2^C$ is the neighborhood function.

Ullman? Sipser? defines NTM as DTM with transition relation instead of transition function.

We define $\beta^n$ as a composition of $n$ instances of $\beta$:
$\beta^0 = \text{id}$ and $\beta^1 = \beta$ and $\beta^n = \beta \circ \beta^{n-1}$.
The time complexity of a computation $x$
is the smallest $n$ such that $\beta^{n+1} x = \beta^n x$.

\section{Major theme in computability theory}

The idea is to encode almost \emph{everything}
(machines, algorithms, configurations, computations, numbers, graphs, you name it)
as a \emph{string} and then establish a bijection between strings and natural numbers.

Time-limited simulation of another machine:
simulate the machine as long as the number of steps do not exceed $f n$.
If the number of step exceeds $f n$ and the machine still does not terminate,
output $0$.
Truncated language?

A particular kind of machine called the \emph{recursive machines}
is equivalent to Turing machines with polynomial-time speedup/slowdown.

For each machine with transition function $t$ there exists another machine
with transition function $t' = t^p$
providing polynomial speedup.
Space usage of configuration $x$.
Length of program plus length of input.

\section{Terminology and notation}

\begin{mdef}[list, string]
    A \emph{list} $x$ of $X$ of length $n$ is $[x_0,x_1,\ldots,x_{n-1}]$
    where each $x_k$ has the type $X$.
    The list $x$ has type $X^*$ (the Kleene closure of $X$).
    The empty list is written $[]$.
    Lists are also known as \emph{strings}.
\end{mdef}

\begin{mdef}[problem, instance, language]
    An \emph{instance} is a string.
    A \emph{problem} is a set of instances.
    A problem is also called a \emph{language}.
\end{mdef}

\begin{mdef}[complexity class]
    A \emph{complexity class} is a set of problems.
\end{mdef}

\chapter{The proof}

\begin{mdef}[time complexity of an expression]
    We write $\TC_M e$ to mean the number of time units required by machine $M$
    to evaluate expression $e$.
    Sometimes $M$ is omitted so we write only $\TC e$
    such as when the machine is not too relevant
    or is clear in the context.

    To evaluate an expression is to reduce it until it becomes a value.
    A value is an expression that reduces to itself.
\end{mdef}

\begin{mdef}[decider]
An \emph{$X$-decider} is an implementation of a function having the type $X \to \mathbb B$.
We say that the decider \emph{accepts} the input $x$ iff $fx$ is true.
The \emph{language} recognized by the decider is the set
\begin{align*}
    \langset f = \{ x ~|~ x : X, ~ fx \}
\end{align*}
\end{mdef}

\begin{mdef}[lower bound]
    We say that $b$ is a time usage lower bound of decider $f$ iff $fx \in \Omega b$.
    We say that $b$ is the time usage infimum of that decider iff
    for all $c$ that is a time usage lower bound of $f$,
    $b$ is a time usage lower bound of $f$ and $b \in \Omega c$.
\end{mdef}

\begin{mdef}[decider set]
    The \emph{decider set} of a language $L$, written $\decset L$, is
    the set of all deciders whose language is $L$.
    \begin{align}
        \decset L = \{ f ~|~ \langset f = L \}
    \end{align}
\end{mdef}

\begin{mdef}[decider set time usage ordering]
    We say $f \leTC g$ iff $\forall x \in X^* ~ \TC(fx) \in O(\TC(gx))$.
\end{mdef}

\begin{mdef}[minimum-time decider]
    $f$ is a \emph{minimum-time decider} of $L$ iff for each $g \in \decset L$,
    it holds that $\forall x \in X^* ~ \TC(fx) \in O(\TC(gx))$.
    In other words, such $f$ is an infimum of $\decset L$ according to $\leTC$.
\end{mdef}

\begin{msco}[nonexistence of time usage supremum]
    $\decset L$ has no supremum according to $\leTC$.
    For each decider $f \in \decset L$, there exists an asymptotically slower $g \in \decset L$.
    \begin{align}
        \forall f \in \decset L ~ \exists g \in \decset L ~ \forall x \in X^* ~ \TC(fx) \in o(gx)
    \end{align}
\end{msco}

\begin{mcor}[decider set equivalence class]
    We can define two deciders as $f$ and $g$
    as equivalent iff for all input $x$,
    their time usage is asymptotically similar.
    \begin{align}
        \mathcal E m p L = \{ f ~|~ \langset f = \decset L, ~ \forall x \in X^* ~ \TC_m(fx) \in \Theta(\TC_m(px)) \}
    \end{align}
\end{mcor}

\begin{msco}[NTIME upper bound]
    Let $N$ be a nondeterministic machine.
    The number of time units required by $N$
    to evaluate $\SDP fx$ is bounded above as follows
    where $n$ is the length of $x$:
    \begin{align}
        \TC_N(\SDP fx) \in O\left(n + \max_{s \sqsubseteq x} (\TC(fs))\right)
    \end{align}
\end{msco}

\begin{msco}[DTIME upper bound]
    Let $D$ be a deterministic machine.
    The number of time units required by $D$
    to evaluate $\SDP fx$ is bounded above as follows
    where $n$ is the length of $x$:
    \begin{align}
        \TC_D(\SDP fx) \in O \left( \sum_{s \sqsubseteq x} \TC(fs) \right)
    \end{align}
\end{msco}

\begin{mdef}[$g$-transformer]
    A \emph{$g$-transformer} of a decider $f$ is a function $g : X^* \to X^*$
    such that $fx = f(gx)$ for all $x : X^*$.
    If $\TC(fx) \in o(\TC(f(gx)))$ then
    we call the $g$-transformer a \emph{$g$-speedup}.
    On the other hand if $\TC(fx) \in \omega(\TC(f(gx)))$ then
    we call the $g$-transformer a \emph{$g$-slowdown}.
\end{mdef}

\begin{mcon}[DTIME lower bound]
    For each deterministic machine $D$,
    $\TC_D(\SDP fx) \in \Omega(m^n)$ for all $f : X^* \to \mathbb B$
    and $x : X^*$
    where $|X| = m$
    and $|x| = n$.
\end{mcon}

\begin{mcon}[existence of a certain decider]
There exists an alphabet $X$
and an $X^*$-decider $f$ such that
for each list $x : X^*$ of length $n$,
the following two statements hold
\begin{align}
    \TC_D(\SDP fx) &\in \Omega 2^n
    \\
    \TC_N(\SDP fx) &\in O n
\end{align}
where $D$ and $N$ are the deterministic machine
and the nondeterministic machine from the two previous conjectures.
If such $f$ exists, then $\PTIME \neq \NPTIME$.
\end{mcon}

\begin{mcon}
    If $f = f \circ g$ for all $f$ then $g = \id$.
\end{mcon}

\begin{mcon}[existence of a PNP decider]
Such decider cannot have overlapping subproblem
and cannot have optimal substructure.
There is no mathematical identity that allows decider to be computed faster.
If
\begin{align}
    \neg\exists p ~ \forall x \in \{ a | a \in X^*, p a \} ~ \exists y ~ fx = fy
\end{align}
then $f$ exists.
\end{mcon}

Proof by contradiction:
suppose that for every decider,
there exists a mathematical identity that allows it to be computed faster.
\begin{align}
    \forall f ~ \exists p ~ \forall x \in \{ a | a \in X^*, p a \} ~ \exists y ~ fx = fy
\end{align}
Then what?

\chapter{Machines}

\begin{mdef}[standard machine]
    The standard machine with alphabet $X$
    has the following primitive operation set:
    
    Every constant of type $X$.

    Every unary operation of type $X \to X$.

    Every binary operation of type $X \to X \to X$.

    Every ternary operation of type $X \to X \to X \to X$.
    One of these ternary operations is the three-way branch operation:
    $pctf$ is $t$ iff $c \neq 0$; $f$ otherwise.

    Function application.

    Function composition.

    Some list operations: $\fnull : X^* \to X$,
    $\fhead : X^* \to X$,
    $\ftail : X^* \to X^*$,
    and $\fcons : X \to X^* \to X^*$.

    A fixed-point combinator $y : \forall a ~ (\alpha \to \alpha) \to \alpha$ satisfying $yf = f(yf)$
    such that the expression $yf$ reduces to $f(yf)$ in one time unit.
\end{mdef}

\section{Recursion}

\begin{mlem}
    The type of the Y-combinator is $\forall a ~ (\alpha \to \alpha) \to \alpha$.
    \begin{proof}
        The definition is $yf = f(yf)$.
        \begin{align}
            f &: \alpha
            \\
            r &: \alpha \to \beta
            \\
            yf &: \beta
            \\
            f &: \beta \to \gamma
            \\
            f(yf) &: \gamma
            \\
            yf : \beta, \ f(yf) : \gamma, \ yf = f(yf) &\implies \beta = \gamma
            \\
            f : \alpha, \ yf : \beta, \ f(yf) : \gamma &\implies f : \beta \to \gamma, \ \alpha = \beta \to \gamma
        \end{align}
    \end{proof}
\end{mlem}

Recursive has to do with fixpoints.

\section{Nondeterminism}

In a nondeterministic machine, $\ambc x y$
has the same effect as reducing $x$ and $y$ in parallel in one unit time.
Let the accepting computation be $z$ where $z$ is either $x$ or $y$.
\begin{align}
    \TC(\ambc xy) &= \TC z
\end{align}

\bibliography{sdp}
\bibliographystyle{plain}

\end{document}
